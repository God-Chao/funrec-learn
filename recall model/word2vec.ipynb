{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-gram算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 超参数\n",
    "embedding_dim = 10  # 词向量维度\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造简单的词汇表和数据\n",
    "corpus = [\"we like deep learning\", \"we love NLP\", \"word embedding is fun\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造词汇表\n",
    "word_list = set(\" \".join(corpus).split())  # 去重后的单词集合\n",
    "word2idx = {word: idx for idx, word in enumerate(word_list)} # 单词到索引的映射\n",
    "idx2word = {idx: word for word, idx in word2idx.items()} # 索引到单词的映射\n",
    "vocab_size = len(word2idx) # 词汇表大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 4],\n",
       "        [0, 6],\n",
       "        [4, 0],\n",
       "        [4, 6],\n",
       "        [4, 8],\n",
       "        [6, 0],\n",
       "        [6, 4],\n",
       "        [6, 8],\n",
       "        [8, 4],\n",
       "        [8, 6],\n",
       "        [0, 9],\n",
       "        [0, 1],\n",
       "        [9, 0],\n",
       "        [9, 1],\n",
       "        [1, 0],\n",
       "        [1, 9],\n",
       "        [5, 2],\n",
       "        [5, 7],\n",
       "        [2, 5],\n",
       "        [2, 7],\n",
       "        [2, 3],\n",
       "        [7, 5],\n",
       "        [7, 2],\n",
       "        [7, 3],\n",
       "        [3, 2],\n",
       "        [3, 7]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成 Skip-gram 训练数据\n",
    "window_size = 2\n",
    "skip_gram_data = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for idx, center_word in enumerate(words):\n",
    "        for offset in range(-window_size, window_size + 1):\n",
    "            if offset == 0 or idx + offset < 0 or idx + offset >= len(words):\n",
    "                continue\n",
    "            context_word = words[idx + offset]\n",
    "            skip_gram_data.append((word2idx[center_word], word2idx[context_word])) # 格式为(中心词索引, 上下文词索引)\n",
    "\n",
    "# 转换为 Tensor\n",
    "skip_gram_data = torch.tensor(skip_gram_data)\n",
    "skip_gram_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Skip-gram 模型\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.WI = nn.Embedding(vocab_size, embedding_dim)  # 中心词向量空间 (V x N)\n",
    "        self.WO = nn.Embedding(vocab_size, embedding_dim)  # 上下文词向量空间 (V x N)\n",
    "\n",
    "    def forward(self, center_idx):\n",
    "        v_c = self.WI(center_idx)  # (batch_size, embedding_dim)\n",
    "        scores = torch.matmul(v_c, self.WO.weight.T)  # (batch_size, vocab_size)\n",
    "        return scores\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SkipGram(vocab_size, embedding_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 164.4450\n",
      "Epoch 10, Loss: 37.0246\n",
      "Epoch 20, Loss: 29.0041\n",
      "Epoch 30, Loss: 27.7177\n",
      "Epoch 40, Loss: 27.2980\n",
      "Epoch 50, Loss: 27.1040\n",
      "Epoch 60, Loss: 26.9948\n",
      "Epoch 70, Loss: 26.9253\n",
      "Epoch 80, Loss: 26.8769\n",
      "Epoch 90, Loss: 26.8411\n",
      "\n",
      "Learned word vectors:\n",
      "we: [-0.39432     0.8618684   0.9984683   1.9259332   0.47502545 -0.37072268\n",
      " -1.118236    0.03735328 -0.27072257 -2.4090092 ]\n",
      "NLP: [-1.4238187  -1.2575887   0.71602035 -0.04907556  2.4831998  -0.80715996\n",
      " -0.54783607 -0.2558084  -1.4790802   0.27846402]\n",
      "embedding: [-0.07708196  0.52656186  0.3533564  -1.1550801  -2.9387343   0.25311458\n",
      "  0.2802841  -0.01401285 -0.71019435  0.07720388]\n",
      "fun: [ 0.24812387  0.19299272  0.41594127 -2.0790749   0.13417944 -0.10976417\n",
      " -2.0473742  -1.6911982  -0.5540867   0.73592556]\n",
      "like: [ 0.09149608 -1.3952626   0.36846802  2.6559212   0.3489073   0.9896139\n",
      "  0.7166462  -1.6571385  -1.2075752   0.23042937]\n",
      "word: [ 0.5310363  -0.38897008  0.49036792 -0.65369684 -1.6652849  -0.24581239\n",
      " -1.5820827  -0.64558846  0.5306749   0.42606357]\n",
      "deep: [-0.55034655 -0.09482031  1.3993858   0.8703548   1.1836715   0.3185585\n",
      "  0.35004786  0.65881884  0.08643673  1.1432636 ]\n",
      "is: [ 0.89765483  1.4100965   0.21100362 -1.4642379  -2.0513568   2.4780908\n",
      "  1.2791005  -0.32777935  0.4883174  -0.711887  ]\n",
      "learning: [ 0.2181462   1.1078036   1.0076777   3.0044346   0.19129068  0.11954059\n",
      " -0.21208969 -0.9425334  -0.4457659  -1.9433234 ]\n",
      "love: [-0.38823238  1.5771155   1.6054304  -1.5615008   2.0605128   1.0423126\n",
      " -0.73529637 -0.40792966 -0.05138374  0.4354028 ]\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for center, context in skip_gram_data:\n",
    "        center, context = center.to(device), context.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scores = model(center.unsqueeze(0))  # 增加 batch 维度\n",
    "        loss = criterion(scores, context.unsqueeze(0))  # 计算损失\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 打印训练后的词向量\n",
    "print(\"\\nLearned word vectors:\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word}: {model.WI.weight[idx].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given center word: we\n",
      "Predicted context word: NLP\n"
     ]
    }
   ],
   "source": [
    "# 获取中心词的索引\n",
    "center_word = 'we'\n",
    "center_idx = word2idx[center_word]\n",
    "center_tensor = torch.tensor([center_idx]).to(device)\n",
    "\n",
    "# 使用模型进行预测\n",
    "with torch.no_grad():\n",
    "    scores = model(center_tensor)\n",
    "    predicted_context_idx = torch.argmax(scores, dim=1).item()\n",
    "    predicted_context_word = idx2word[predicted_context_idx]\n",
    "\n",
    "print(f'Given center word: {center_word}')\n",
    "print(f'Predicted context word: {predicted_context_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语料库\n",
    "corpus = [\"The cat sits on the mat\"]\n",
    "words = corpus[0].split()  # 分词\n",
    "word2idx = {word: i for i, word in enumerate(set(words))}  # 词汇表\n",
    "idx2word = {i: word for word, i in word2idx.items()}  # 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([2, 4]), tensor(0)),\n",
       " (tensor([0, 4, 1]), tensor(2)),\n",
       " (tensor([0, 2, 1, 5]), tensor(4)),\n",
       " (tensor([2, 4, 5, 3]), tensor(1)),\n",
       " (tensor([4, 1, 3]), tensor(5)),\n",
       " (tensor([1, 5]), tensor(3))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成 CBOW 训练数据\n",
    "window_size = 2\n",
    "cbow_data = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for idx, center_word in enumerate(words):\n",
    "        context_words = []\n",
    "        for offset in range(-window_size, window_size + 1):\n",
    "            if offset == 0 or idx + offset < 0 or idx + offset >= len(words):\n",
    "                continue\n",
    "            context_words.append(word2idx[words[idx + offset]])  # 记录上下文词索引\n",
    "        if context_words:  # 避免空列表\n",
    "            cbow_data.append((context_words, word2idx[center_word]))  # (上下文词索引列表, 目标词索引)\n",
    "\n",
    "# 转换为 Tensor\n",
    "cbow_data = [(torch.tensor(context), torch.tensor(target)) for context, target in cbow_data]\n",
    "cbow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 CBOW 模型\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.WI = nn.Embedding(vocab_size, embedding_dim)  # 上下文词向量空间 (V x N)\n",
    "        self.WO = nn.Embedding(vocab_size, embedding_dim)  # 中心词向量空间 (V x N)\n",
    "\n",
    "    def forward(self, context_idxs):\n",
    "        context_embeddings = self.WI(context_idxs)\n",
    "        average_emb = torch.mean(context_embeddings, dim=0, keepdim=True)\n",
    "        scores = torch.matmul(average_emb, self.WO.weight.T)\n",
    "        return scores\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CBOW(vocab_size, embedding_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 13.0367\n",
      "Epoch 10, Loss: 5.9836\n",
      "Epoch 20, Loss: 3.2476\n",
      "Epoch 30, Loss: 1.6222\n",
      "Epoch 40, Loss: 0.8533\n",
      "Epoch 50, Loss: 0.5131\n",
      "Epoch 60, Loss: 0.3419\n",
      "Epoch 70, Loss: 0.2445\n",
      "Epoch 80, Loss: 0.1838\n",
      "Epoch 90, Loss: 0.1433\n",
      "\n",
      "Learned word vectors:\n",
      "The: [ 0.8771341  0.6165877 -1.2195442  0.0255321 -1.1571873  3.2178388\n",
      " -1.4898465  0.1290686 -0.4132271  1.014745 ]\n",
      "on: [ 0.04589905  0.24310425 -0.11267089 -0.24023066 -2.347304   -0.18171461\n",
      " -1.3725036  -0.13852957  0.8322103  -0.47539952]\n",
      "cat: [ 0.27115706  1.0187441   0.611658   -1.5865067   0.87147933  0.5577066\n",
      "  0.62968713 -0.3058953   0.62642515  0.26221675]\n",
      "mat: [-1.1161038  -0.7870448   1.2830974   2.3960016  -0.24700347  2.0023594\n",
      " -1.1109082  -2.7715046  -0.09689652  1.1125424 ]\n",
      "sits: [-0.89836186  0.99905235  0.7934982  -0.5487706   0.22069451  0.47899535\n",
      "  0.6576656  -1.7366772  -2.9801214  -0.66734016]\n",
      "the: [ 2.212858   -1.3035347   1.319773    1.3719721   0.6784411  -0.11297243\n",
      "  1.7667874   2.1787608  -1.1562164   1.2123032 ]\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for context, center in cbow_data:\n",
    "        context, center = context.to(device), center.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scores = model(context)  # 增加 batch 维度\n",
    "        loss = criterion(scores, center.unsqueeze(0))  # 计算损失\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 打印训练后的词向量\n",
    "print(\"\\nLearned word vectors:\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word}: {model.WI.weight[idx].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given context words: ['sits', 'on']\n",
      "Predicted center word: cat\n"
     ]
    }
   ],
   "source": [
    "# 获取上下文词的索引\n",
    "context_words = ['sits', 'on']\n",
    "context_idxs = [word2idx[word] for word in context_words]\n",
    "context_tensor = torch.tensor([context_idxs]).to(device)\n",
    "\n",
    "# 使用模型进行预测\n",
    "with torch.no_grad():\n",
    "    scores = model(context_tensor.squeeze())\n",
    "    predicted_center_idx = torch.argmax(scores, dim=1).item()\n",
    "    predicted_center_word = idx2word[predicted_center_idx]\n",
    "\n",
    "print(f'Given context words: {context_words}')\n",
    "print(f'Predicted center word: {predicted_center_word}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
