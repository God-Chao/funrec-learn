{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据准备\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat and the dog are friends\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构建词汇表\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "word_counts = Counter(word for sentence in tokenized_corpus for word in sentence) # 每个单词以及出现的次数\n",
    "vocab = {word: i for i, word in enumerate(word_counts.keys())} # 单词到索引的映射\n",
    "vocab_size = len(vocab) # 词汇表大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 2),\n",
       " (1, 0),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 3),\n",
       " (2, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 0),\n",
       " (3, 4),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (4, 3),\n",
       " (4, 0),\n",
       " (0, 5),\n",
       " (0, 2),\n",
       " (5, 0),\n",
       " (5, 2),\n",
       " (5, 3),\n",
       " (2, 0),\n",
       " (2, 5),\n",
       " (2, 3),\n",
       " (2, 0),\n",
       " (3, 5),\n",
       " (3, 2),\n",
       " (3, 0),\n",
       " (3, 6),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 6),\n",
       " (6, 3),\n",
       " (6, 0),\n",
       " (0, 1),\n",
       " (0, 7),\n",
       " (1, 0),\n",
       " (1, 7),\n",
       " (1, 0),\n",
       " (7, 0),\n",
       " (7, 1),\n",
       " (7, 0),\n",
       " (7, 5),\n",
       " (0, 1),\n",
       " (0, 7),\n",
       " (0, 5),\n",
       " (0, 8),\n",
       " (5, 7),\n",
       " (5, 0),\n",
       " (5, 8),\n",
       " (5, 9),\n",
       " (8, 0),\n",
       " (8, 5),\n",
       " (8, 9),\n",
       " (9, 5),\n",
       " (9, 8)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 生成 (中心词, 上下文词) 训练样本\n",
    "window_size = 2\n",
    "training_data = []\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    for i, center_word in enumerate(sentence):\n",
    "        center_idx = vocab[center_word]\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j != 0 and 0 <= i + j < len(sentence):\n",
    "                context_word = sentence[i + j]\n",
    "                training_data.append((center_idx, vocab[context_word])) # (中心词索引, 上下文词索引)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Skip-gram 模型定义\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, center_word):\n",
    "        center_embedding = self.embeddings(center_word)\n",
    "        scores = self.output_layer(center_embedding)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 训练模型\n",
    "embedding_dim = 10\n",
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 92.1898\n",
      "Epoch 40/100, Loss: 91.0246\n",
      "Epoch 60/100, Loss: 90.6198\n",
      "Epoch 80/100, Loss: 90.3839\n",
      "Epoch 100/100, Loss: 90.2198\n",
      "Word: the, Vector: [-0.7219838  -0.06475896 -0.7327847  -0.08595231 -1.6121358  -0.08662665\n",
      " -0.23153545  0.04500464  0.4962679  -1.0625919 ]\n",
      "Word: cat, Vector: [ 1.213518    2.2492442   1.3218019   0.62236327 -0.0303225   0.94328797\n",
      "  0.08740252 -0.7029813   0.8023205  -0.09477841]\n",
      "Word: sat, Vector: [ 2.9422662  -0.27853212 -0.46484956  0.41288215 -0.67734915 -0.04913354\n",
      "  1.574669   -0.49794376  0.3392137   0.6131823 ]\n",
      "Word: on, Vector: [ 0.92557365 -0.8099594  -0.17121041 -0.52117527  2.0382535   0.34503347\n",
      " -1.3556825  -0.7853026   0.36183363 -0.10263178]\n",
      "Word: mat, Vector: [ 0.87801206 -0.47459403  1.5125083   3.232189    1.2623043   0.11753491\n",
      "  2.4696615   1.2696218   0.35343483  0.92886215]\n",
      "Word: dog, Vector: [-0.39919415  1.8964784   0.9231417   0.9305452   0.45523068 -0.6378382\n",
      " -0.15911524  0.02091936 -0.26834136  1.16201   ]\n",
      "Word: log, Vector: [ 1.8338912   0.7063154   1.395299    1.8440676  -0.0315856  -0.27014217\n",
      "  2.6523588  -0.92607594 -0.255016   -0.1220317 ]\n",
      "Word: and, Vector: [ 2.0095975  -0.8765442  -1.914261   -0.42578408  0.69464815  0.5197611\n",
      "  1.5479963  -2.3361993  -0.14405045  0.9130424 ]\n",
      "Word: are, Vector: [ 0.7815579   0.8233416   0.1752743  -1.3999106  -0.9114136   2.676103\n",
      "  1.2782171   0.48537537 -1.7186823   2.4212077 ]\n",
      "Word: friends, Vector: [ 0.56399757  1.6072165  -3.151438   -1.2631989   0.12031792 -1.7568345\n",
      "  0.31711307  1.7149854  -0.52534634  0.9503694 ]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for center, context in training_data:\n",
    "        center_tensor = torch.tensor([center], dtype=torch.long)\n",
    "        context_tensor = torch.tensor([context], dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(center_tensor)\n",
    "        loss = criterion(output, context_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 6. 输出词向量\n",
    "word_vectors = model.embeddings.weight.data.numpy()\n",
    "for word, idx in vocab.items():\n",
    "    print(f\"Word: {word}, Vector: {word_vectors[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
